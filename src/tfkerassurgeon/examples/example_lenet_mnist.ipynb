{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "example_lenet_mnist.ipynb",
      "version": "0.3.2",
      "provenance": [],
      "collapsed_sections": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "code",
      "metadata": {
        "id": "Dan1HjbGDaWo",
        "colab_type": "code",
        "outputId": "c46b5034-8d16-44ce-e961-40740f1f2357",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 710
        }
      },
      "source": [
        "! pip uninstall -y tensorflow\n",
        "! pip uninstall -y tf-nightly\n",
        "\n",
        "! pip install -U tf-nightly-gpu\n",
        "\n",
        "! pip install tfkerassurgeon\n"
      ],
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Uninstalling tensorflow-1.14.0rc1:\n",
            "  Successfully uninstalled tensorflow-1.14.0rc1\n",
            "\u001b[33mWARNING: Skipping tf-nightly as it is not installed.\u001b[0m\n",
            "Collecting tf-nightly-gpu\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/84/fe/82ab4aac9be6d8ab50dcb76a8c1710f030d1eb274688fe87492f9d73a3f9/tf_nightly_gpu-1.14.1.dev20190622-cp36-cp36m-manylinux1_x86_64.whl (394.6MB)\n",
            "\u001b[K     |████████████████████████████████| 394.6MB 57kB/s \n",
            "\u001b[?25hCollecting tb-nightly<1.15.0a0,>=1.14.0a0 (from tf-nightly-gpu)\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/3a/60/afa129c3621d62c885599076f8e89737d66e5dfffad1a08842b1c11b4540/tb_nightly-1.14.0a20190614-py3-none-any.whl (3.1MB)\n",
            "\u001b[K     |████████████████████████████████| 3.2MB 46.7MB/s \n",
            "\u001b[?25hCollecting tf-estimator-nightly (from tf-nightly-gpu)\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/f1/fb/94605ff7f1b94ca5d2845104a2fcd37270ae294d6992635fb4476d79364c/tf_estimator_nightly-1.14.0.dev2019062201-py2.py3-none-any.whl (498kB)\n",
            "\u001b[K     |████████████████████████████████| 501kB 47.4MB/s \n",
            "\u001b[?25hRequirement already satisfied, skipping upgrade: google-pasta>=0.1.6 in /usr/local/lib/python3.6/dist-packages (from tf-nightly-gpu) (0.1.7)\n",
            "Requirement already satisfied, skipping upgrade: keras-applications>=1.0.8 in /usr/local/lib/python3.6/dist-packages (from tf-nightly-gpu) (1.0.8)\n",
            "Requirement already satisfied, skipping upgrade: gast>=0.2.0 in /usr/local/lib/python3.6/dist-packages (from tf-nightly-gpu) (0.2.2)\n",
            "Requirement already satisfied, skipping upgrade: termcolor>=1.1.0 in /usr/local/lib/python3.6/dist-packages (from tf-nightly-gpu) (1.1.0)\n",
            "Requirement already satisfied, skipping upgrade: protobuf>=3.6.1 in /usr/local/lib/python3.6/dist-packages (from tf-nightly-gpu) (3.7.1)\n",
            "Requirement already satisfied, skipping upgrade: wrapt>=1.11.1 in /usr/local/lib/python3.6/dist-packages (from tf-nightly-gpu) (1.11.1)\n",
            "Requirement already satisfied, skipping upgrade: six>=1.10.0 in /usr/local/lib/python3.6/dist-packages (from tf-nightly-gpu) (1.12.0)\n",
            "Requirement already satisfied, skipping upgrade: wheel>=0.26 in /usr/local/lib/python3.6/dist-packages (from tf-nightly-gpu) (0.33.4)\n",
            "Requirement already satisfied, skipping upgrade: grpcio>=1.8.6 in /usr/local/lib/python3.6/dist-packages (from tf-nightly-gpu) (1.15.0)\n",
            "Requirement already satisfied, skipping upgrade: absl-py>=0.7.0 in /usr/local/lib/python3.6/dist-packages (from tf-nightly-gpu) (0.7.1)\n",
            "Requirement already satisfied, skipping upgrade: numpy<2.0,>=1.14.5 in /usr/local/lib/python3.6/dist-packages (from tf-nightly-gpu) (1.16.4)\n",
            "Requirement already satisfied, skipping upgrade: astor>=0.6.0 in /usr/local/lib/python3.6/dist-packages (from tf-nightly-gpu) (0.8.0)\n",
            "Requirement already satisfied, skipping upgrade: keras-preprocessing>=1.0.5 in /usr/local/lib/python3.6/dist-packages (from tf-nightly-gpu) (1.1.0)\n",
            "Requirement already satisfied, skipping upgrade: markdown>=2.6.8 in /usr/local/lib/python3.6/dist-packages (from tb-nightly<1.15.0a0,>=1.14.0a0->tf-nightly-gpu) (3.1.1)\n",
            "Requirement already satisfied, skipping upgrade: werkzeug>=0.11.15 in /usr/local/lib/python3.6/dist-packages (from tb-nightly<1.15.0a0,>=1.14.0a0->tf-nightly-gpu) (0.15.4)\n",
            "Requirement already satisfied, skipping upgrade: setuptools>=41.0.0 in /usr/local/lib/python3.6/dist-packages (from tb-nightly<1.15.0a0,>=1.14.0a0->tf-nightly-gpu) (41.0.1)\n",
            "Requirement already satisfied, skipping upgrade: h5py in /usr/local/lib/python3.6/dist-packages (from keras-applications>=1.0.8->tf-nightly-gpu) (2.8.0)\n",
            "Installing collected packages: tb-nightly, tf-estimator-nightly, tf-nightly-gpu\n",
            "Successfully installed tb-nightly-1.14.0a20190614 tf-estimator-nightly-1.14.0.dev2019062201 tf-nightly-gpu-1.14.1.dev20190622\n",
            "Collecting tfkerassurgeon\n",
            "  Downloading https://files.pythonhosted.org/packages/73/87/c8109bacaf9c922a718456ac0ae688e5c9cf920918ea3e7c6dd7451a993d/tfkerassurgeon-0.2.0.tar.gz\n",
            "Building wheels for collected packages: tfkerassurgeon\n",
            "  Building wheel for tfkerassurgeon (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Stored in directory: /root/.cache/pip/wheels/60/83/7a/8560c6394f0bc8097d0b8e83d3fa4d5562ac62f7b0730d68ff\n",
            "Successfully built tfkerassurgeon\n",
            "Installing collected packages: tfkerassurgeon\n",
            "Successfully installed tfkerassurgeon-0.2.0\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "7d-JgZUzUWpP",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        },
        "outputId": "7c256905-6b52-4289-d514-105cfebd09c8"
      },
      "source": [
        "\n",
        "import tensorflow as tf\n",
        "\n",
        "from tensorflow.python.keras import layers\n",
        "from tensorflow.python.keras import activations\n",
        "from tensorflow.python.keras import models\n",
        "from tensorflow.python.keras import backend\n",
        "from tensorflow.python.keras import callbacks\n",
        "from tensorflow.examples.tutorials import mnist\n",
        "\n",
        "import tfkerassurgeon\n",
        "from tfkerassurgeon import identify\n",
        "from tfkerassurgeon.operations import delete_channels\n",
        "\n",
        "print(tf.__version__)\n"
      ],
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "1.14.1-dev20190622\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "XfwJo-gYz2-N",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "\n",
        "# Set some static values that can be tweaked to experiment\n",
        "keras_verbosity = 2 # limits the printed output but still gets the Epoch stats\n",
        "epochs=200 # we'd never reach 200 because we have early stopping\n",
        "batch_size=128 # tweak this depending on your hardware and Model\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "6rM72tZez3B5",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 570
        },
        "outputId": "bc95b534-ad9d-4847-e3a9-da51c640c486"
      },
      "source": [
        "\n",
        "# Load dataset (it will automatically download it if needed)\n",
        "dataset = mnist.input_data.read_data_sets('tempData', one_hot=True, reshape=False)\n",
        "\n"
      ],
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "WARNING: Logging before flag parsing goes to stderr.\n",
            "W0622 20:13:57.683336 139665831700352 deprecation.py:323] From <ipython-input-4-275b5aaa04e1>:1: read_data_sets (from tensorflow.contrib.learn.python.learn.datasets.mnist) is deprecated and will be removed in a future version.\n",
            "Instructions for updating:\n",
            "Please use alternatives such as official/mnist/dataset.py from tensorflow/models.\n",
            "W0622 20:13:57.685291 139665831700352 deprecation.py:323] From /usr/local/lib/python3.6/dist-packages/tensorflow_core/contrib/learn/python/learn/datasets/mnist.py:260: maybe_download (from tensorflow.contrib.learn.python.learn.datasets.base) is deprecated and will be removed in a future version.\n",
            "Instructions for updating:\n",
            "Please write your own downloading logic.\n",
            "W0622 20:13:57.686792 139665831700352 deprecation.py:323] From /usr/local/lib/python3.6/dist-packages/tensorflow_core/contrib/learn/python/learn/datasets/base.py:252: _internal_retry.<locals>.wrap.<locals>.wrapped_fn (from tensorflow.contrib.learn.python.learn.datasets.base) is deprecated and will be removed in a future version.\n",
            "Instructions for updating:\n",
            "Please use urllib or similar directly.\n",
            "W0622 20:13:58.102969 139665831700352 deprecation.py:323] From /usr/local/lib/python3.6/dist-packages/tensorflow_core/contrib/learn/python/learn/datasets/mnist.py:262: extract_images (from tensorflow.contrib.learn.python.learn.datasets.mnist) is deprecated and will be removed in a future version.\n",
            "Instructions for updating:\n",
            "Please use tf.data to implement this functionality.\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "Successfully downloaded train-images-idx3-ubyte.gz 9912422 bytes.\n",
            "Extracting tempData/train-images-idx3-ubyte.gz\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "W0622 20:13:58.501961 139665831700352 deprecation.py:323] From /usr/local/lib/python3.6/dist-packages/tensorflow_core/contrib/learn/python/learn/datasets/mnist.py:267: extract_labels (from tensorflow.contrib.learn.python.learn.datasets.mnist) is deprecated and will be removed in a future version.\n",
            "Instructions for updating:\n",
            "Please use tf.data to implement this functionality.\n",
            "W0622 20:13:58.507020 139665831700352 deprecation.py:323] From /usr/local/lib/python3.6/dist-packages/tensorflow_core/contrib/learn/python/learn/datasets/mnist.py:110: dense_to_one_hot (from tensorflow.contrib.learn.python.learn.datasets.mnist) is deprecated and will be removed in a future version.\n",
            "Instructions for updating:\n",
            "Please use tf.one_hot on tensors.\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "Successfully downloaded train-labels-idx1-ubyte.gz 28881 bytes.\n",
            "Extracting tempData/train-labels-idx1-ubyte.gz\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "W0622 20:13:58.902250 139665831700352 deprecation.py:323] From /usr/local/lib/python3.6/dist-packages/tensorflow_core/contrib/learn/python/learn/datasets/mnist.py:290: DataSet.__init__ (from tensorflow.contrib.learn.python.learn.datasets.mnist) is deprecated and will be removed in a future version.\n",
            "Instructions for updating:\n",
            "Please use alternatives such as official/mnist/dataset.py from tensorflow/models.\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "Successfully downloaded t10k-images-idx3-ubyte.gz 1648877 bytes.\n",
            "Extracting tempData/t10k-images-idx3-ubyte.gz\n",
            "Successfully downloaded t10k-labels-idx1-ubyte.gz 4542 bytes.\n",
            "Extracting tempData/t10k-labels-idx1-ubyte.gz\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ku8uWZ4az3Gf",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "\n",
        "# Simple reusable shorthand to compile the model, so that we can be sure to use the same optomizer, loss, and metrics\n",
        "def compile_model(model):\n",
        "    \n",
        "    model.compile(optimizer='adam',\n",
        "                    loss='categorical_crossentropy',\n",
        "                    metrics=['accuracy'])\n",
        "\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "X1_m2nNez26i",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "\n",
        "# method that encapsulates the Models archeteture and construction\n",
        "def build_model():\n",
        "\n",
        "    # Create LeNet model\n",
        "    model = models.Sequential()\n",
        "    model.add(layers.Conv2D(20,\n",
        "                     [3, 3],\n",
        "                     input_shape=[28, 28, 1],\n",
        "                     activation='relu',\n",
        "                     name='conv_1'))\n",
        "    model.add(layers.MaxPool2D())\n",
        "    model.add(layers.Conv2D(50, [3, 3], activation='relu', name='conv_2'))\n",
        "    model.add(layers.MaxPool2D())\n",
        "    model.add(layers.Permute((2, 1, 3)))\n",
        "    model.add(layers.Flatten())\n",
        "    model.add(layers.Dense(500, activation='relu', name='dense_1'))\n",
        "    model.add(layers.Dense(10, activation='softmax', name='dense_2'))\n",
        "\n",
        "    compile_model(model)\n",
        "\n",
        "    return model\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "4EueuGY1z_ug",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        },
        "outputId": "a726cdfb-1632-4005-9caf-5ed0310f09a6"
      },
      "source": [
        "\n",
        "# a simple method that gets the callbacks for training\n",
        "def get_callbacks(use_early_stopping = True, use_reduce_lr = True):\n",
        "\n",
        "    callback_list = []\n",
        "\n",
        "    if(use_early_stopping):\n",
        "\n",
        "        callback_list.append(callbacks.EarlyStopping(monitor='val_loss',\n",
        "                                             min_delta=0,\n",
        "                                             patience=10,\n",
        "                                             verbose=keras_verbosity,\n",
        "                                             mode='auto'))\n",
        "\n",
        "    if(use_reduce_lr):\n",
        "\n",
        "        callback_list.append(callbacks.ReduceLROnPlateau(monitor='val_loss',\n",
        "                                            factor=0.1,\n",
        "                                            patience=5,\n",
        "                                            verbose=keras_verbosity,\n",
        "                                            mode='auto',\n",
        "                                            epsilon=0.0001,\n",
        "                                            cooldown=0,\n",
        "                                            min_lr=0))\n",
        "\n",
        "    return callback_list\n",
        "\n",
        "# and get the callbacks\n",
        "callback_list = get_callbacks()"
      ],
      "execution_count": 7,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "W0622 20:13:59.174589 139665831700352 callbacks.py:1780] `epsilon` argument is deprecated and will be removed, use `min_delta` instead.\n"
          ],
          "name": "stderr"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "lF7DbGaAz_xP",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "\n",
        "# Simple reusable shorthand for evaluating the model on the Validation set \n",
        "def fit_model(model):\n",
        "    \n",
        "    return model.fit(\n",
        "                    dataset.train.images,\n",
        "                    dataset.train.labels,\n",
        "                    epochs=epochs,\n",
        "                    batch_size=batch_size,\n",
        "                    verbose=keras_verbosity,\n",
        "                    validation_data=(dataset.validation.images, dataset.validation.labels),\n",
        "                    callbacks=callback_list)\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "98A1fJqcz_1E",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "\n",
        "# Simple reusable shorthand for evaluating the model on the Validation set \n",
        "def eval_model(model):\n",
        "\n",
        "    return model.evaluate(\n",
        "                        dataset.validation.images, \n",
        "                        dataset.validation.labels, \n",
        "                        batch_size=batch_size, \n",
        "                        verbose=keras_verbosity)\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "cUKa3ANfz_8A",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "# THIS IS WHERE THE MAGIC HAPPENS!\n",
        "# This method uses the Keras Surgeon to identify which parts od a layer can be pruned and then deletes them\n",
        "# Note: it returns the new, pruned model, that was recompiled\n",
        "def prune_layer(model, layer):\n",
        "    \n",
        "    # Get the APOZ (Average Percentage of Zeros) that should identify where we can prune\n",
        "    apoz = identify.get_apoz(model, layer, dataset.validation.images)\n",
        "\n",
        "    # Get the Channel Ids that have a high APOZ, which indicates they can be pruned\n",
        "    high_apoz_channels = identify.high_apoz(apoz)\n",
        "\n",
        "    # Run the pruning on the Model and get the Pruned (uncompiled) model as a result\n",
        "    model = delete_channels(model, layer, high_apoz_channels)\n",
        "\n",
        "    # Recompile the model\n",
        "    compile_model(model)\n",
        "\n",
        "    return model\n",
        "\n",
        "  \n",
        "  \n",
        "  \n",
        "  \n",
        "  \n",
        "  \n",
        "  \n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "OWaCTapg0IlY",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "\n",
        "# A helper that gets the layer by it's name \n",
        "def prune_layer_by_name(model, layer_name):\n",
        "\n",
        "    # First we get the layer we are working on\n",
        "    layer = model.get_layer(name=layer_name)\n",
        "    # Then prune is and return the pruned model\n",
        "    return prune_layer(model, layer)\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "7-6fPJQt0In-",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "\n",
        "# the main function, that runs the training\n",
        "def main(): \n",
        "\n",
        "    \n",
        "    # build the model\n",
        "    model = build_model()\n",
        "\n",
        "    # Initial Train on dataset\n",
        "    results = fit_model(model)\n",
        "\n",
        "    # eval and print the results of the training\n",
        "    loss = eval_model(model)\n",
        "    print('original model loss:', loss, '\\n')\n",
        "    \n",
        "    # NOTE: This while true will continue until it ERRORs out because there is no escape condition.\n",
        "    while True:\n",
        "\n",
        "        # only prune the Dense layer for this example\n",
        "        layer_name = 'dense_1'\n",
        "        # Run the Pruning on the layer\n",
        "        model = prune_layer_by_name(model, layer_name)\n",
        "\n",
        "        # eval and print the results of the pruning\n",
        "        loss = eval_model(model)\n",
        "        print('model loss after pruning: ', loss, '\\n')\n",
        "        \n",
        "        # Retrain the model to accomodate for the changes\n",
        "        results = fit_model(model)\n",
        "\n",
        "        # eval and print the results of the retraining\n",
        "        loss = eval_model(model)\n",
        "        print('model loss after retraining: ', loss, '\\n')\n",
        "\n",
        "        # While TRUE will repeat until an ERROR occurs\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "bjEpABn8DyLN",
        "colab_type": "code",
        "outputId": "ca36bdde-6095-4af8-8d71-5a36abe4060b",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1474
        }
      },
      "source": [
        "\n",
        "# Run the main Method\n",
        "if __name__ == '__main__':\n",
        "    main()\n"
      ],
      "execution_count": 13,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "W0622 20:13:59.275877 139665831700352 deprecation.py:506] From /usr/local/lib/python3.6/dist-packages/tensorflow_core/python/ops/resource_variable_ops.py:1624: calling BaseResourceVariable.__init__ (from tensorflow.python.ops.resource_variable_ops) with constraint is deprecated and will be removed in a future version.\n",
            "Instructions for updating:\n",
            "If using Keras pass *_constraint arguments to layers.\n",
            "W0622 20:14:00.337045 139665831700352 deprecation.py:323] From /usr/local/lib/python3.6/dist-packages/tensorflow_core/python/keras/optimizer_v2/optimizer_v2.py:460: BaseResourceVariable.constraint (from tensorflow.python.ops.resource_variable_ops) is deprecated and will be removed in a future version.\n",
            "Instructions for updating:\n",
            "Apply a constraint manually following the optimizer update step.\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "Train on 55000 samples, validate on 5000 samples\n",
            "Epoch 1/200\n",
            "55000/55000 - 6s - loss: 0.2078 - acc: 0.9395 - val_loss: 0.0760 - val_acc: 0.9770\n",
            "Epoch 2/200\n",
            "55000/55000 - 2s - loss: 0.0545 - acc: 0.9833 - val_loss: 0.0448 - val_acc: 0.9878\n",
            "Epoch 3/200\n",
            "55000/55000 - 2s - loss: 0.0374 - acc: 0.9884 - val_loss: 0.0357 - val_acc: 0.9904\n",
            "Epoch 4/200\n",
            "55000/55000 - 2s - loss: 0.0275 - acc: 0.9914 - val_loss: 0.0322 - val_acc: 0.9898\n",
            "Epoch 5/200\n",
            "55000/55000 - 2s - loss: 0.0215 - acc: 0.9931 - val_loss: 0.0434 - val_acc: 0.9874\n",
            "Epoch 6/200\n",
            "55000/55000 - 2s - loss: 0.0163 - acc: 0.9944 - val_loss: 0.0308 - val_acc: 0.9914\n",
            "Epoch 7/200\n",
            "55000/55000 - 2s - loss: 0.0136 - acc: 0.9955 - val_loss: 0.0317 - val_acc: 0.9902\n",
            "Epoch 8/200\n",
            "55000/55000 - 2s - loss: 0.0101 - acc: 0.9969 - val_loss: 0.0336 - val_acc: 0.9902\n",
            "Epoch 9/200\n",
            "55000/55000 - 2s - loss: 0.0087 - acc: 0.9972 - val_loss: 0.0352 - val_acc: 0.9918\n",
            "Epoch 10/200\n",
            "55000/55000 - 2s - loss: 0.0082 - acc: 0.9972 - val_loss: 0.0409 - val_acc: 0.9906\n",
            "Epoch 11/200\n",
            "\n",
            "Epoch 00011: ReduceLROnPlateau reducing learning rate to 0.00010000000474974513.\n",
            "55000/55000 - 2s - loss: 0.0070 - acc: 0.9977 - val_loss: 0.0312 - val_acc: 0.9916\n",
            "Epoch 12/200\n",
            "55000/55000 - 2s - loss: 0.0020 - acc: 0.9994 - val_loss: 0.0272 - val_acc: 0.9926\n",
            "Epoch 13/200\n",
            "55000/55000 - 2s - loss: 0.0010 - acc: 0.9998 - val_loss: 0.0272 - val_acc: 0.9932\n",
            "Epoch 14/200\n",
            "55000/55000 - 2s - loss: 7.2214e-04 - acc: 0.9999 - val_loss: 0.0277 - val_acc: 0.9932\n",
            "Epoch 15/200\n",
            "55000/55000 - 2s - loss: 5.4532e-04 - acc: 0.9999 - val_loss: 0.0285 - val_acc: 0.9936\n",
            "Epoch 16/200\n",
            "55000/55000 - 2s - loss: 4.5463e-04 - acc: 1.0000 - val_loss: 0.0291 - val_acc: 0.9938\n",
            "Epoch 17/200\n",
            "\n",
            "Epoch 00017: ReduceLROnPlateau reducing learning rate to 1.0000000474974514e-05.\n",
            "55000/55000 - 2s - loss: 3.6568e-04 - acc: 1.0000 - val_loss: 0.0292 - val_acc: 0.9932\n",
            "Epoch 18/200\n",
            "55000/55000 - 2s - loss: 2.8190e-04 - acc: 1.0000 - val_loss: 0.0292 - val_acc: 0.9932\n",
            "Epoch 19/200\n",
            "55000/55000 - 2s - loss: 2.6904e-04 - acc: 1.0000 - val_loss: 0.0292 - val_acc: 0.9934\n",
            "Epoch 20/200\n",
            "55000/55000 - 2s - loss: 2.6288e-04 - acc: 1.0000 - val_loss: 0.0293 - val_acc: 0.9934\n",
            "Epoch 21/200\n",
            "55000/55000 - 2s - loss: 2.5397e-04 - acc: 1.0000 - val_loss: 0.0294 - val_acc: 0.9934\n",
            "Epoch 22/200\n",
            "\n",
            "Epoch 00022: ReduceLROnPlateau reducing learning rate to 1.0000000656873453e-06.\n",
            "55000/55000 - 2s - loss: 2.4527e-04 - acc: 1.0000 - val_loss: 0.0296 - val_acc: 0.9932\n",
            "Epoch 00022: early stopping\n",
            "5000/5000 - 0s - loss: 0.0296 - acc: 0.9932\n",
            "original model loss: [0.029591922644991428, 0.9932] \n",
            "\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "W0622 20:14:46.153893 139665831700352 deprecation_wrapper.py:118] From /usr/local/lib/python3.6/dist-packages/tensorflow_estimator/python/estimator/api/_v1/estimator/__init__.py:10: The name tf.estimator.inputs is deprecated. Please use tf.compat.v1.estimator.inputs instead.\n",
            "\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "error",
          "ename": "TypeError",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-13-c7bc734e5e35>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0;32mif\u001b[0m \u001b[0m__name__\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;34m'__main__'\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 2\u001b[0;31m     \u001b[0mmain\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
            "\u001b[0;32m<ipython-input-12-044715f6f8d8>\u001b[0m in \u001b[0;36mmain\u001b[0;34m()\u001b[0m\n\u001b[1;32m     18\u001b[0m         \u001b[0mlayer_name\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m'dense_1'\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     19\u001b[0m         \u001b[0;31m# Run the Pruning on the layer\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 20\u001b[0;31m         \u001b[0mmodel\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mprune_layer_by_name\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmodel\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlayer_name\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     21\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     22\u001b[0m         \u001b[0;31m# eval and print the results of the pruning\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m<ipython-input-11-d4c647c1383f>\u001b[0m in \u001b[0;36mprune_layer_by_name\u001b[0;34m(model, layer_name)\u001b[0m\n\u001b[1;32m      4\u001b[0m     \u001b[0mlayer\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmodel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget_layer\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mname\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mlayer_name\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m     \u001b[0;31m# Then prune is and return the pruned model\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 6\u001b[0;31m     \u001b[0;32mreturn\u001b[0m \u001b[0mprune_layer\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmodel\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlayer\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
            "\u001b[0;32m<ipython-input-10-90b6e2831d3f>\u001b[0m in \u001b[0;36mprune_layer\u001b[0;34m(model, layer)\u001b[0m\n\u001b[1;32m      8\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      9\u001b[0m     \u001b[0;31m# Run the pruning on the Model and get the Pruned (uncompiled) model as a result\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 10\u001b[0;31m     \u001b[0mmodel\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mdelete_channels\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmodel\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlayer\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mhigh_apoz_channels\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     11\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     12\u001b[0m     \u001b[0;31m# Recompile the model\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/tfkerassurgeon/operations.py\u001b[0m in \u001b[0;36mdelete_channels\u001b[0;34m(model, layer, channels, node_indices, copy)\u001b[0m\n\u001b[1;32m    103\u001b[0m     \u001b[0msurgeon\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mSurgeon\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmodel\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcopy\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    104\u001b[0m     \u001b[0msurgeon\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0madd_job\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'delete_channels'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlayer\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnode_indices\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mnode_indices\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mchannels\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mchannels\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 105\u001b[0;31m     \u001b[0;32mreturn\u001b[0m \u001b[0msurgeon\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0moperate\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/tfkerassurgeon/surgeon.py\u001b[0m in \u001b[0;36moperate\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    150\u001b[0m         \u001b[0;32mfor\u001b[0m \u001b[0mnode\u001b[0m \u001b[0;32min\u001b[0m \u001b[0msorted_nodes\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    151\u001b[0m             \u001b[0;31m# Rebuild submodel up to this node\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 152\u001b[0;31m             \u001b[0msub_output_nodes\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mutils\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget_node_inbound_nodes\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mnode\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    153\u001b[0m             outputs, output_masks = self._rebuild_graph(self.model.inputs,\n\u001b[1;32m    154\u001b[0m                                                         sub_output_nodes)\n",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/tfkerassurgeon/utils.py\u001b[0m in \u001b[0;36mget_node_inbound_nodes\u001b[0;34m(node)\u001b[0m\n\u001b[1;32m     88\u001b[0m \u001b[0;32mdef\u001b[0m \u001b[0mget_node_inbound_nodes\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mnode\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     89\u001b[0m     return [get_inbound_nodes(node.inbound_layers[i])[node_index]\n\u001b[0;32m---> 90\u001b[0;31m             for i, node_index in enumerate(node.node_indices)]\n\u001b[0m\u001b[1;32m     91\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     92\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mTypeError\u001b[0m: 'int' object is not iterable"
          ]
        }
      ]
    }
  ]
}